[{"authors":["admin"],"categories":null,"content":"Hi I\u0026rsquo;m Michael! I\u0026rsquo;m interested in machine learning, particularly the Bayesian flavour.\nRecently I\u0026rsquo;ve worked on Architecture Search of Bayesian Neural Networks, and Differential Privacy for Federated and Continual Bayesian Learning.\nBroadly I\u0026rsquo;m interested in statistically principled machine learning. I\u0026rsquo;m working on pushing the theoretical boundaries of this, and help make it useful in the real world! Things that I think are interesting currently include approximate inference for large models (such as variational inference), validation methods for these approximations (such as uncertainty quantification), and generative modelling.\nMy interests aren\u0026rsquo;t completely settled however, and I\u0026rsquo;m always keen to explore new areas. Reinforcement Learning is the next on my todo list.\nI recently started a PhD course at the University of Oxford through the StatML course, supervised by Yee Whye Teh. Before that I completed a Masters of Engineering at the University of Cambridge, supervised by Dr Rich E. Turner.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mjhutchinson.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi I\u0026rsquo;m Michael! I\u0026rsquo;m interested in machine learning, particularly the Bayesian flavour.\nRecently I\u0026rsquo;ve worked on Architecture Search of Bayesian Neural Networks, and Differential Privacy for Federated and Continual Bayesian Learning.\nBroadly I\u0026rsquo;m interested in statistically principled machine learning. I\u0026rsquo;m working on pushing the theoretical boundaries of this, and help make it useful in the real world! Things that I think are interesting currently include approximate inference for large models (such as variational inference), validation methods for these approximations (such as uncertainty quantification), and generative modelling.","tags":null,"title":"Michael Hutchinson","type":"authors"},{"authors":null,"categories":null,"content":"Today marks my first day at the Unversity of Oxford, starting a PhD in Statisitcal Machine Learning.\nI\u0026rsquo;m joining the StatML course run in the statistics department. The first year consists of two parts: A mix of taught courses covering a range of topics from classical statistical techniques, all the way through to modern deep learning, and a series of small reaserch projects. The aim of the course is to give students with a range of interests some extra time and skills to settle on a good PhD topic. The three years after the first function as a normal PhD program, but with some additional cohort courses and events.\nThrought this I will be a student under Yee Whye Teh, co-supervised by Max Welling. I\u0026rsquo;m massivly looking forawrd to working with them on a range of exciting topics, kicking things off with a project on Distributed Bayesian Learning!\n","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"9a65c3fdd7d03bdd71a4eaa6ca484293","permalink":"https://mjhutchinson.github.io/news/statml_2019_begin/","publishdate":"2019-10-06T00:00:00Z","relpermalink":"/news/statml_2019_begin/","section":"news","summary":"Today marks my first day at the University of Oxford, starting a PhD in statistical Machine Learning ...","tags":null,"title":"Started at the University of Oxford","type":"news"},{"authors":null,"categories":null,"content":"Our paper Differnetial Private Federated Variational Inference has been accepted at the Neurips Privacy in Machine Learning workshop in Vancouver. Come say hello at the poster session if you\u0026rsquo;d like to discuss it!\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"34266be2e311c3cd9f94b132ffdd83d1","permalink":"https://mjhutchinson.github.io/news/neurips_2019_diff_priv/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/news/neurips_2019_diff_priv/","section":"news","summary":"Our paper *Differnetial Private Federated Variational Inference* has been accepted at the Neurips Privacy in Machine Learning workshop in Vancouver. Come say hello at the poster session if you'd like to discuss it!","tags":null,"title":"First paper accepted at NeurIPS Privacy in Machine Learning Workshop!","type":"news"},{"authors":["Michael Hutchinson"],"categories":[],"content":" Modern machine learning techniques are quickly being applied to a vast range of problems in the real world. However, most modern techniques are not well suited to handle a number of difficult situations. A recent piece of work we did aims to combine approaches for tackling three of these.\nSituations where we would like uncertainty estimates in our predictions For tasks such as classifying each photo in you photo library into a number of categories, or tagging them each as a particular person, the downsides to getting the decision wrong are minimal, if annoying for the end user. There are many other settings where the downsides to making the incorrect choice could mean life or death, such as medical applications, or driverless cars.\nIn these situations, we would like to know exactly how confident our model is in its predictions or decisions, so that downstream systems can account for this when taking action. For example when a driverless car is planning how to turn right, we\u0026rsquo;d very much like to know if it\u0026rsquo;s 99% sure that nothing is in its way, or if its a 50\u0026frasl;50 toss up between that and a small child.\nThis kind of problem can be solved with a more principled application of statistics to Machine Learning. When applied to deep learning this is often refereed to as Bayesian Deep Learning.\nThere exists a large body of literature in this area. In this project we focused on Variational Inference (VI) techniques.1\nSituations where the data we wish to learn from isn\u0026rsquo;t in one place It is easy to imagine that for a number of reasons, the data we want our model to learn from cannot be bought to a single locations. Examples may include legal reasons (e.g. the recent GDPR in Europe), or practical reasons (such as mobile phone/edge device data, or data that is too large to be efficiently transported).\nIn these situations, we would still like to be able to learn from this data, but many techniques cannot naively handle this. This area of work again has a large body of work, and is know as Distributed or Federated Learning.\nSituations where the data we are handling is privacy-sensitive Much of the data that we would like to use to train Machine Learning models is sensitive to someone in some way. This could be medical data, browsing history, or financial data. Data sources (i.e. the individuals from whom this data is collected) are rightly skeptical about handing over their information to third parities.\nRecent work showed that deep learning models have a tendency to memorise their input data, and using targeted attacks some input data can be recovered from them.2 In this case, the threat to a users sensitive data comes not just from people to whom they hand their data, but from anyone who has access to the models trained on their data.\nDifferential Privacy3 is the current gold standard in protecting the privacy of data while extracting information from it. By carefully processing updates, and in particular by adding noise, we can achieve a series of guarantees regarding information leakage, user utility loss, and more.\nDiffernetial Private Federated Variational Inference Our recent work builds on the Partition Variational Inference4 framework, a Variational Inference framework that allows for easy distributed and continual learning, and extends this to include privacy preserving updates based on recent differential privacy literature. There will be an arxiv preprint available soon. In the meantime, check out the code, and feel free to get in touch with any questions!\nUPDATE: This paper has been accepted to the Privacy in Machine Learning workshop at Neurips 2019!\n Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. \u0026ldquo;Variational inference: A review for statisticians.\u0026rdquo; Journal of the American statistical Association 112.518 (2017): 859-877. arxiv ^ Carlini, Nicholas, et al. \u0026ldquo;The secret sharer: Measuring unintended neural network memorization \u0026amp; extracting secrets.\u0026rdquo; arXiv preprint arXiv:1802.08232 (2018). arxiv ^ Dwork, Cynthia, and Aaron Roth. \u0026ldquo;The algorithmic foundations of differential privacy.\u0026rdquo; Foundations and Trends® in Theoretical Computer Science 9.3–4 (2014): 211-407. website ^ Bui, Thang D., et al. \u0026ldquo;Partitioned Variational Inference: A unified framework encompassing federated and continual learning.\u0026rdquo; arXiv preprint arXiv:1811.11206 (2018). arxiv ^   ","date":1569628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569628800,"objectID":"cef96dd73e186f5592bf6840082252a6","permalink":"https://mjhutchinson.github.io/project/2019-10-01-differentially-private-pvi/","publishdate":"2019-09-28T00:00:00Z","relpermalink":"/project/2019-10-01-differentially-private-pvi/","section":"project","summary":"Learning Private, Bayesian Machine Learning Models in the Federating Learning Context","tags":[],"title":"Differential Privacy, Approximate Bayesian Inference and Distributed Learning","type":"project"},{"authors":["Mrinank Sharma*","**Michael Hutchinson***","Siddharth Swaroop","Antti Honkela","Richard E Turner"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2253d7e3c06b9872d020f52d6cda862f","permalink":"https://mjhutchinson.github.io/publication/sharma-2019/","publishdate":"2020-01-17T11:50:37.61972Z","relpermalink":"/publication/sharma-2019/","section":"publication","summary":"In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on. Furthermore, each client's data, computational resources and communication constraints may be very different. This setting is known as federated learning, in which privacy is a key concern. Differential privacy is commonly used to provide mathematical privacy guarantees. This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning. We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting. We modify the client-side optimisation of PVI to provide an ($backslashepsilon$, $backslashdelta$)-DP guarantee. We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.","tags":null,"title":"Differentially Private Federated Variational Inference","type":"publication"}]