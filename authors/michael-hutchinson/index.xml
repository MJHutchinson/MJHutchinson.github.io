<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Michael Hutchinson</title>
    <link>http://localhost:1313/authors/michael-hutchinson/</link>
      <atom:link href="http://localhost:1313/authors/michael-hutchinson/index.xml" rel="self" type="application/rss+xml" />
    <description>Michael Hutchinson</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 27 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/img/icon-192.png</url>
      <title>Michael Hutchinson</title>
      <link>http://localhost:1313/authors/michael-hutchinson/</link>
    </image>
    
    <item>
      <title>Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge Equivariant Projected Kernels</title>
      <link>http://localhost:1313/publication/hutchinson-2021-gauge/</link>
      <pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/hutchinson-2021-gauge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Bayesian Inference of Instantaneous Re-production Numbers at Fine Spatial Scales, with an Application to Mapping and Nowcasting the Covid-19 Epidemic in British Local Authorities</title>
      <link>http://localhost:1313/publication/teh-2021-efficient/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/teh-2021-efficient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LieTransformer: Equivariant self-attention for Lie Groups</title>
      <link>http://localhost:1313/publication/hutchinson-2020-lietransformer/</link>
      <pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/hutchinson-2020-lietransformer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes </title>
      <link>http://localhost:1313/publication/holderrieth-2020-equivariant/</link>
      <pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/holderrieth-2020-equivariant/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Age groups that sustain resurging COVID-19 epidemics in the United States</title>
      <link>http://localhost:1313/publication/monod-2020-09-18-20197376/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/monod-2020-09-18-20197376/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning Summer School 2020</title>
      <link>http://localhost:1313/post/2020-06-25-mlss/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2020-06-25-mlss/</guid>
      <description>&lt;h2 id=&#34;poster-presentation&#34;&gt;Poster presentation&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/xP5vsUuVyKg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/files/mlss/poster-presentation-slides.pdf&#34;&gt;Slides for this presentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;papers-mentioned-in-this-talk&#34;&gt;Papers mentioned in this talk&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Steerable CNNs&lt;/strong&gt;. Cohen, T. S., &amp;amp; Welling, M. (2016). &lt;a href=&#34;http://arxiv.org/abs/1612.08498&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spherical CNNs&lt;/strong&gt;. Cohen, T. S., Geiger, M., KÃ¶hler, J., &amp;amp; Welling, M. (2018). &lt;a href=&#34;https://arxiv.org/abs/1801.10130v3&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General $E(2)$-Equivariant Steerable CNNs&lt;/strong&gt;. Weiler, M., &amp;amp; Cesa, G. (2019). &lt;a href=&#34;http://arxiv.org/abs/1911.08251&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A General Theory of Equivariant CNNs on Homogeneous Spaces&lt;/strong&gt;. Cohen, T., Geiger, M., &amp;amp; Weiler, M. (2018). &lt;a href=&#34;http://arxiv.org/abs/1811.02017&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gauge Equivariant Convolutional Networks and the Icosahedral CNN&lt;/strong&gt;. Cohen, T. S., Weiler, M., Kicanaoglu, B., &amp;amp; Welling, M. (2019).  &lt;a href=&#34;http://arxiv.org/abs/1902.04615&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data&lt;/strong&gt;. Finzi, M., Stanton, S., Izmailov, P., &amp;amp; Wilson, A. G. (2020). &lt;a href=&#34;http://arxiv.org/abs/2002.12880&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds&lt;/strong&gt;. Thomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L., Kohlhoff, K., &amp;amp; Riley, P. (2018).  &lt;a href=&#34;http://arxiv.org/abs/1802.08219&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks&lt;/strong&gt;. Fuchs, F. B., Worrall, D. E., Fischer, V., &amp;amp; Welling, M. (2020). &lt;a href=&#34;http://arxiv.org/abs/2006.10503&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attentive Group Equivariant Convolutional Networks&lt;/strong&gt;. Romero, D. W., Bekkers, E. J., Tomczak, J. M., &amp;amp; Hoogendoorn, M. (2020). &lt;a href=&#34;http://arxiv.org/abs/2002.03830&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Technical Document 3: Effectiveness and Resource Requirements of Test, Trace and Isolate Strategies</title>
      <link>http://localhost:1313/publication/he-2020-technical/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/he-2020-technical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>State-level tracking of COVID-19 in the United States</title>
      <link>http://localhost:1313/publication/unwin-2020/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/unwin-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Report 21: Estimating COVID-19 cases and reproduction number in Brazil</title>
      <link>http://localhost:1313/publication/mellan-2020-report/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/mellan-2020-report/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A sub-national analysis of the rate of transmission of Covid-19 in Italy</title>
      <link>http://localhost:1313/publication/vollmer-2020-report/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/vollmer-2020-report/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentially Private Federated Variational Inference</title>
      <link>http://localhost:1313/publication/sharma-2019/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/sharma-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differential Privacy, Approximate Bayesian Inference and Distributed Learning</title>
      <link>http://localhost:1313/post/2019-10-01-differentially-private-pvi/</link>
      <pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2019-10-01-differentially-private-pvi/</guid>
      <description>&lt;p&gt;Modern machine learning techniques are quickly being applied to a vast range of problems in the real world. However, most modern techniques are not well suited to handle a number of difficult situations. A recent piece of work we did aims to combine approaches for tackling three of these.&lt;/p&gt;
&lt;h3 id=&#34;situations-where-we-would-like-uncertainty-estimates-in-our-predictions&#34;&gt;Situations where we would like uncertainty estimates in our predictions&lt;/h3&gt;
&lt;p&gt;For tasks such as classifying each photo in you photo library into a number of categories, or tagging them each as a particular person, the downsides to getting the decision wrong are minimal, if annoying for the end user. There are many other settings where the downsides to making the incorrect choice could mean life or death, such as medical applications, or driverless cars.&lt;/p&gt;
&lt;p&gt;In these situations, we would like to know exactly how confident our model is in its predictions or decisions, so that downstream systems can account for this when taking action. For example when a driverless car is planning how to turn right, we&amp;rsquo;d very much like to know if it&amp;rsquo;s 99% sure that nothing is in its way, or if its a 50/50 toss up between that and a small child.&lt;/p&gt;
&lt;p&gt;This kind of problem can be solved with a more principled application of statistics to Machine Learning. When applied to deep learning this is often refereed to as &lt;em&gt;&lt;a href=&#34;http://bayesiandeeplearning.org/&#34;&gt;Bayesian Deep Learning&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There exists a large body of literature in this area. In this project we focused on Variational Inference (VI) techniques.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&#34;situations-where-the-data-we-wish-to-learn-from-isnt-in-one-place&#34;&gt;Situations where the data we wish to learn from isn&amp;rsquo;t in one place&lt;/h3&gt;
&lt;p&gt;It is easy to imagine that for a number of reasons, the data we want our model to learn from cannot be bought to a single locations. Examples may include legal reasons (e.g. the recent &lt;a href=&#34;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&#34;&gt;GDPR&lt;/a&gt; in Europe), or practical reasons (such as mobile phone/edge device data, or data that is too large to be efficiently transported).&lt;/p&gt;
&lt;p&gt;In these situations, we would still like to be able to learn from this data, but many techniques cannot naively handle this. This area of work again has a large body of work, and is know as &lt;em&gt;Distributed or Federated Learning&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;situations-where-the-data-we-are-handling-is-privacy-sensitive&#34;&gt;Situations where the data we are handling is privacy-sensitive&lt;/h3&gt;
&lt;p&gt;Much of the data that we would like to use to train Machine Learning models is sensitive to someone in some way. This could be medical data, browsing history, or financial data. Data sources (i.e. the individuals from whom this data is collected) are rightly skeptical about handing over their information to third parities.&lt;/p&gt;
&lt;p&gt;Recent work showed that deep learning models have a tendency to memorise their input data, and using targeted attacks some input data can be recovered from them.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; In this case, the threat to a users sensitive data comes not just from people to whom they hand their data, but from anyone who has access to the models trained on their data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Differential Privacy&lt;/em&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; is the current gold standard in protecting the privacy of data while extracting information from it. By carefully processing updates, and in particular by adding noise, we can achieve a series of guarantees regarding information leakage, user utility loss, and more.&lt;/p&gt;
&lt;h3 id=&#34;differnetial-private-federated-variational-inference&#34;&gt;Differnetial Private Federated Variational Inference&lt;/h3&gt;
&lt;p&gt;Our recent work builds on the Partition Variational Inference&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; framework, a Variational Inference framework that allows for easy distributed and continual learning, and extends this to include privacy preserving updates based on recent differential privacy literature. There will be an arxiv preprint available soon. In the meantime, check out the &lt;a href=&#34;github.com/MrinankSharma/DP-PVI&#34;&gt;code&lt;/a&gt;, and feel free to get in touch with any questions!&lt;/p&gt;
&lt;p&gt;UPDATE: This paper has been accepted to the &lt;a href=&#34;https://priml-workshop.github.io/priml2019/&#34;&gt;Privacy in Machine Learning workshop&lt;/a&gt; at Neurips 2019!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &amp;ldquo;Variational inference: A review for statisticians.&amp;rdquo; Journal of the American statistical Association 112.518 (2017): 859-877. &lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;&gt;arxiv&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Carlini, Nicholas, et al. &amp;ldquo;The secret sharer: Measuring unintended neural network memorization &amp;amp; extracting secrets.&amp;rdquo; arXiv preprint arXiv:1802.08232 (2018). &lt;a href=&#34;https://arxiv.org/abs/1802.08232&#34;&gt;arxiv&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Dwork, Cynthia, and Aaron Roth. &amp;ldquo;The algorithmic foundations of differential privacy.&amp;rdquo; Foundations and TrendsÂ® in Theoretical Computer Science 9.3â4 (2014): 211-407. &lt;a href=&#34;https://www.cis.upenn.edu/~aaroth/privacybook.html&#34;&gt;website&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Bui, Thang D., et al. &amp;ldquo;Partitioned Variational Inference: A unified framework encompassing federated and continual learning.&amp;rdquo; arXiv preprint arXiv:1811.11206 (2018). &lt;a href=&#34;https://arxiv.org/abs/1811.11206&#34;&gt;arxiv&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
